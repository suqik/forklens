{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11a7297e-18d7-44b3-95f9-12ff66656d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from forklens import train\n",
    "from forklens.dataset import ShapeDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "from astropy.io import fits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb85ced5-5d6b-4aa2-bb2d-b9b1126b17e5",
   "metadata": {},
   "source": [
    "Here we train the CNN with 100,000 pairs of mock galaxy and PSF pairs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c602ba1-d779-480a-9997-89dd307cb667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data catalog\n",
    "with fits.open('../../data/csst_snr_catalog.fits') as f:\n",
    "    cat = f[0].data\n",
    "cut_idx = np.where((cat[:,3]>0.1)&(cat[:,4]<25))[0]\n",
    "size_cat = cat[cut_idx,3]\n",
    "mag_cat = cat[cut_idx,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a08fd3a-63fd-4745-ab9c-1cb03c12ba25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "def dataframe(num, seed=12345):\n",
    "    \n",
    "    num = int(num/2)\n",
    "    \n",
    "    rng1 = np.random.RandomState(seed)\n",
    "    idx = rng1.randint(0,mag_cat.shape[0],size=num)\n",
    "    Gal_Hlr   = size_cat[idx]\n",
    "    Gal_Hlr   = np.concatenate((Gal_Hlr, Gal_Hlr), axis=0)\n",
    "    Gal_Mag   = mag_cat[idx]\n",
    "    Gal_Mag   = np.concatenate((Gal_Mag, Gal_Mag), axis=0)\n",
    "    \n",
    "    rng2 = np.random.RandomState(seed+1)\n",
    "    Gal_Theta = rng2.random(num*2)*2*np.pi\n",
    "    Gal_ELL = np.sqrt(rng2.random(num*2))*0.999\n",
    "    e1 = Gal_ELL*np.sin(2*Gal_Theta)\n",
    "    e2 = Gal_ELL*np.cos(2*Gal_Theta)\n",
    "\n",
    "    rng3 = np.random.RandomState(seed+2)\n",
    "    PSF_randint = rng3.randint(0,10000,size=num)\n",
    "    PSF_randint = np.concatenate((PSF_randint, PSF_randint), axis=0)\n",
    "    \n",
    "    gal_pars = {}\n",
    "    gal_pars[\"e1\"] = e1\n",
    "    gal_pars[\"e2\"] = e2\n",
    "    gal_pars[\"hlr_disk\"] = Gal_Hlr\n",
    "    gal_pars[\"mag_i\"] = Gal_Mag\n",
    "    \n",
    "    psf_pars = {}\n",
    "    psf_pars['randint'] = PSF_randint\n",
    "    \n",
    "    return gal_pars, psf_pars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e11fa9-262c-45f2-99fa-1d218b5f691c",
   "metadata": {},
   "source": [
    "In the above we generate a input catalog of the training dataset (including both training and validation).\n",
    "\n",
    "Note two catalogs are generated: one for galaxies and one for PSFs. The catalog for PSFs is simply random index which does not make a difference here, as the simulated PSF image in this code is unchanged for simplicity. \n",
    "\n",
    "One may want to rewrite his own code for simulation.py and also to customize dataset.py.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8ba1dc3-19cf-4672-b735-798c6bacd16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4720952,)\n"
     ]
    }
   ],
   "source": [
    "# Get data loader\n",
    "nSims = 100000\n",
    "GalCat, PSFCat = dataframe(nSims, seed=12345)\n",
    "train_ds = ShapeDataset(GalCat, PSFCat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071ca3cb-db1b-4294-8789-a1fbfccf017b",
   "metadata": {},
   "source": [
    "Some hyper parameters for training are to be modified in ./config.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e98d01d2-4069-4717-9a7f-2023be7a6f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_dl: 360 Validation_dl: 40\n",
      "Begin training ...\n",
      "[TRAIN] Epoch: 1 Loss: 0.27965382910751724 Time: 1:14\n",
      "[VALID] Epoch: 1 Loss: 0.2313229511027656 Time: 0:11\n",
      "[TRAIN] Epoch: 2 Loss: 0.22804690249232595 Time: 1:16\n",
      "[VALID] Epoch: 2 Loss: 0.22015504617240078 Time: 0:11\n",
      "[TRAIN] Epoch: 3 Loss: 0.22308680501612704 Time: 1:18\n",
      "[VALID] Epoch: 3 Loss: 0.22393668344096215 Time: 0:11\n",
      "[TRAIN] Epoch: 4 Loss: 0.21994440234572382 Time: 1:17\n",
      "[VALID] Epoch: 4 Loss: 0.21574010761141812 Time: 0:11\n",
      "[TRAIN] Epoch: 5 Loss: 0.21813957438734216 Time: 1:18\n",
      "[VALID] Epoch: 5 Loss: 0.21390902532403008 Time: 0:11\n",
      "[TRAIN] Epoch: 6 Loss: 0.21584955123358815 Time: 1:17\n",
      "[VALID] Epoch: 6 Loss: 0.21409474290142302 Time: 0:12\n",
      "[TRAIN] Epoch: 7 Loss: 0.21499654557658984 Time: 1:17\n",
      "[VALID] Epoch: 7 Loss: 0.21471007284832788 Time: 0:11\n",
      "[TRAIN] Epoch: 8 Loss: 0.21382891788195516 Time: 1:18\n",
      "[VALID] Epoch: 8 Loss: 0.2141304829640659 Time: 0:12\n",
      "[TRAIN] Epoch: 9 Loss: 0.2116843066521367 Time: 1:18\n",
      "[VALID] Epoch: 9 Loss: 0.21150733639028707 Time: 0:11\n",
      "[TRAIN] Epoch: 10 Loss: 0.21153840155606904 Time: 1:17\n",
      "[VALID] Epoch: 10 Loss: 0.2116328423657865 Time: 0:11\n",
      "[TRAIN] Epoch: 11 Loss: 0.21095573511796697 Time: 1:18\n",
      "[VALID] Epoch: 11 Loss: 0.2093559718203824 Time: 0:11\n",
      "[TRAIN] Epoch: 12 Loss: 0.21010204380159883 Time: 1:17\n",
      "[VALID] Epoch: 12 Loss: 0.2128408599136288 Time: 0:11\n",
      "[TRAIN] Epoch: 13 Loss: 0.21024271915627457 Time: 1:17\n",
      "[VALID] Epoch: 13 Loss: 0.2100673151001363 Time: 0:12\n",
      "[TRAIN] Epoch: 14 Loss: 0.2091244131206056 Time: 1:18\n",
      "[VALID] Epoch: 14 Loss: 0.2065591656982777 Time: 0:11\n",
      "[TRAIN] Epoch: 15 Loss: 0.20977675711085964 Time: 1:17\n",
      "[VALID] Epoch: 15 Loss: 0.2100070858296373 Time: 0:11\n",
      "[TRAIN] Epoch: 16 Loss: 0.20950613003866464 Time: 1:18\n",
      "[VALID] Epoch: 16 Loss: 0.20767962058909709 Time: 0:11\n",
      "[TRAIN] Epoch: 17 Loss: 0.2076683395381882 Time: 1:17\n",
      "[VALID] Epoch: 17 Loss: 0.20788244669301914 Time: 0:12\n",
      "[TRAIN] Epoch: 18 Loss: 0.20798420486048072 Time: 1:17\n",
      "[VALID] Epoch: 18 Loss: 0.21104026339881263 Time: 0:11\n",
      "[TRAIN] Epoch: 19 Loss: 0.20730620890115942 Time: 1:17\n",
      "[VALID] Epoch: 19 Loss: 0.20925025782461018 Time: 0:12\n",
      "[TRAIN] Epoch: 20 Loss: 0.20838954996560743 Time: 1:17\n",
      "[VALID] Epoch: 20 Loss: 0.20571780326803024 Time: 0:11\n",
      "[TRAIN] Epoch: 21 Loss: 0.2070062047126409 Time: 1:17\n",
      "[VALID] Epoch: 21 Loss: 0.20848417047002984 Time: 0:12\n",
      "[TRAIN] Epoch: 22 Loss: 0.2066266612769532 Time: 1:18\n",
      "[VALID] Epoch: 22 Loss: 0.2056195543768884 Time: 0:11\n",
      "[TRAIN] Epoch: 23 Loss: 0.20707193907971794 Time: 1:17\n",
      "[VALID] Epoch: 23 Loss: 0.20579171575371866 Time: 0:11\n",
      "[TRAIN] Epoch: 24 Loss: 0.2064464807652757 Time: 1:17\n",
      "[VALID] Epoch: 24 Loss: 0.20777025873521263 Time: 0:11\n",
      "[TRAIN] Epoch: 25 Loss: 0.20600506055348064 Time: 1:19\n",
      "[VALID] Epoch: 25 Loss: 0.20665007197377716 Time: 0:11\n",
      "[TRAIN] Epoch: 26 Loss: 0.20630110037791866 Time: 1:17\n",
      "[VALID] Epoch: 26 Loss: 0.20603306665933038 Time: 0:11\n",
      "[TRAIN] Epoch: 27 Loss: 0.20653259309149521 Time: 1:17\n",
      "[VALID] Epoch: 27 Loss: 0.2040108160717708 Time: 0:11\n",
      "[TRAIN] Epoch: 28 Loss: 0.20553451123523778 Time: 1:18\n",
      "[VALID] Epoch: 28 Loss: 0.20544084034856794 Time: 0:11\n",
      "[TRAIN] Epoch: 29 Loss: 0.20547243510280913 Time: 1:17\n",
      "[VALID] Epoch: 29 Loss: 0.20869489277741868 Time: 0:11\n",
      "[TRAIN] Epoch: 30 Loss: 0.20570724895928633 Time: 1:18\n",
      "[VALID] Epoch: 30 Loss: 0.20663958267100463 Time: 0:11\n",
      "[TRAIN] Epoch: 31 Loss: 0.20569670225876957 Time: 1:17\n",
      "[VALID] Epoch: 31 Loss: 0.20334288071513557 Time: 0:11\n",
      "[TRAIN] Epoch: 32 Loss: 0.20522272934446936 Time: 1:17\n",
      "[VALID] Epoch: 32 Loss: 0.20689318299698234 Time: 0:12\n",
      "[TRAIN] Epoch: 33 Loss: 0.20487745442883043 Time: 1:17\n",
      "[VALID] Epoch: 33 Loss: 0.20548444527521934 Time: 0:12\n",
      "[TRAIN] Epoch: 34 Loss: 0.2056912681900787 Time: 1:18\n",
      "[VALID] Epoch: 34 Loss: 0.20200990569477517 Time: 0:11\n",
      "[TRAIN] Epoch: 35 Loss: 0.20582109155035067 Time: 1:18\n",
      "[VALID] Epoch: 35 Loss: 0.2015883042947186 Time: 0:12\n",
      "[TRAIN] Epoch: 36 Loss: 0.20547539407042795 Time: 1:18\n",
      "[VALID] Epoch: 36 Loss: 0.2029977062606211 Time: 0:11\n",
      "[TRAIN] Epoch: 37 Loss: 0.20540634325798418 Time: 1:17\n",
      "[VALID] Epoch: 37 Loss: 0.2058193798816914 Time: 0:11\n",
      "[TRAIN] Epoch: 38 Loss: 0.20415691320301468 Time: 1:18\n",
      "[VALID] Epoch: 38 Loss: 0.20516405893689463 Time: 0:11\n",
      "[TRAIN] Epoch: 39 Loss: 0.20437575065805952 Time: 1:17\n",
      "[VALID] Epoch: 39 Loss: 0.20429804643874994 Time: 0:11\n",
      "[TRAIN] Epoch: 40 Loss: 0.20484396660495288 Time: 1:17\n",
      "[VALID] Epoch: 40 Loss: 0.2050934084238146 Time: 0:11\n",
      "[TRAIN] Epoch: 41 Loss: 0.20410320227074302 Time: 1:17\n",
      "[VALID] Epoch: 41 Loss: 0.20385581718315782 Time: 0:11\n",
      "[TRAIN] Epoch: 42 Loss: 0.2036458313160235 Time: 1:17\n",
      "[VALID] Epoch: 42 Loss: 0.2072633088256019 Time: 0:11\n",
      "[TRAIN] Epoch: 43 Loss: 0.204918595536757 Time: 1:17\n",
      "[VALID] Epoch: 43 Loss: 0.20145489519734042 Time: 0:11\n",
      "[TRAIN] Epoch: 44 Loss: 0.20451567301026397 Time: 1:17\n",
      "[VALID] Epoch: 44 Loss: 0.2022830337670151 Time: 0:11\n",
      "[TRAIN] Epoch: 45 Loss: 0.20482196938073066 Time: 1:17\n",
      "[VALID] Epoch: 45 Loss: 0.20374041940135532 Time: 0:11\n",
      "[TRAIN] Epoch: 46 Loss: 0.20452492209128917 Time: 1:17\n",
      "[VALID] Epoch: 46 Loss: 0.20377241041017277 Time: 0:11\n",
      "[TRAIN] Epoch: 47 Loss: 0.2041298785789316 Time: 1:18\n",
      "[VALID] Epoch: 47 Loss: 0.20384710365725628 Time: 0:11\n",
      "[TRAIN] Epoch: 48 Loss: 0.20460505310139424 Time: 1:18\n",
      "[VALID] Epoch: 48 Loss: 0.20358939728288752 Time: 0:11\n",
      "[TRAIN] Epoch: 49 Loss: 0.2038643462070462 Time: 1:18\n",
      "[VALID] Epoch: 49 Loss: 0.2064600384611052 Time: 0:11\n",
      "[TRAIN] Epoch: 50 Loss: 0.20451554649060336 Time: 1:17\n",
      "[VALID] Epoch: 50 Loss: 0.20299852197831625 Time: 0:11\n",
      "[TRAIN] Epoch: 51 Loss: 0.20340784254664876 Time: 1:18\n",
      "[VALID] Epoch: 51 Loss: 0.20197746929774768 Time: 0:11\n",
      "[TRAIN] Epoch: 52 Loss: 0.20430774581794456 Time: 1:17\n",
      "[VALID] Epoch: 52 Loss: 0.20208760004889362 Time: 0:11\n",
      "[TRAIN] Epoch: 53 Loss: 0.20436276942259618 Time: 1:18\n",
      "[VALID] Epoch: 53 Loss: 0.20378768459042154 Time: 0:11\n",
      "[TRAIN] Epoch: 54 Loss: 0.20431929952873115 Time: 1:17\n",
      "[VALID] Epoch: 54 Loss: 0.2029465092872875 Time: 0:11\n",
      "[TRAIN] Epoch: 55 Loss: 0.2036696982369021 Time: 1:18\n",
      "[VALID] Epoch: 55 Loss: 0.20171735404977226 Time: 0:12\n",
      "[TRAIN] Epoch: 56 Loss: 0.2041763900738263 Time: 1:18\n",
      "[VALID] Epoch: 56 Loss: 0.20106456866829134 Time: 0:11\n",
      "[TRAIN] Epoch: 57 Loss: 0.2032824233973009 Time: 1:18\n",
      "[VALID] Epoch: 57 Loss: 0.20303933281153755 Time: 0:12\n",
      "[TRAIN] Epoch: 58 Loss: 0.2042971259699703 Time: 1:18\n",
      "[VALID] Epoch: 58 Loss: 0.2012632138335716 Time: 0:11\n",
      "[TRAIN] Epoch: 59 Loss: 0.20331326093893531 Time: 1:20\n",
      "[VALID] Epoch: 59 Loss: 0.20503240961345184 Time: 0:11\n",
      "[TRAIN] Epoch: 60 Loss: 0.20407688127515855 Time: 1:19\n",
      "[VALID] Epoch: 60 Loss: 0.20549292191043278 Time: 0:11\n",
      "[TRAIN] Epoch: 61 Loss: 0.203718605709432 Time: 1:18\n",
      "[VALID] Epoch: 61 Loss: 0.20359218394346132 Time: 0:12\n",
      "[TRAIN] Epoch: 62 Loss: 0.20351935868764698 Time: 1:18\n",
      "[VALID] Epoch: 62 Loss: 0.2003441208702996 Time: 0:11\n",
      "[TRAIN] Epoch: 63 Loss: 0.20284875176274 Time: 1:19\n",
      "[VALID] Epoch: 63 Loss: 0.20337741097695813 Time: 0:11\n",
      "[TRAIN] Epoch: 64 Loss: 0.20250751828229682 Time: 1:17\n",
      "[VALID] Epoch: 64 Loss: 0.20261071704739367 Time: 0:11\n",
      "[TRAIN] Epoch: 65 Loss: 0.203315065991905 Time: 1:18\n",
      "[VALID] Epoch: 65 Loss: 0.20412063988593265 Time: 0:11\n",
      "[TRAIN] Epoch: 66 Loss: 0.20340544177048528 Time: 1:19\n",
      "[VALID] Epoch: 66 Loss: 0.2016973477351885 Time: 0:11\n",
      "[TRAIN] Epoch: 67 Loss: 0.20345558094089877 Time: 1:16\n",
      "[VALID] Epoch: 67 Loss: 0.2037554281318357 Time: 0:11\n",
      "[TRAIN] Epoch: 68 Loss: 0.20281927343576847 Time: 1:18\n",
      "[VALID] Epoch: 68 Loss: 0.20215047498956973 Time: 0:11\n",
      "[TRAIN] Epoch: 69 Loss: 0.20309760369216617 Time: 1:17\n",
      "[VALID] Epoch: 69 Loss: 0.20099950252619372 Time: 0:12\n",
      "[TRAIN] Epoch: 70 Loss: 0.20251744436825395 Time: 1:17\n",
      "[VALID] Epoch: 70 Loss: 0.20048279764059426 Time: 0:11\n",
      "[TRAIN] Epoch: 71 Loss: 0.2031944891674337 Time: 1:17\n",
      "[VALID] Epoch: 71 Loss: 0.20161593485747695 Time: 0:11\n",
      "[TRAIN] Epoch: 72 Loss: 0.2022092153875908 Time: 1:18\n",
      "[VALID] Epoch: 72 Loss: 0.2039492793054515 Time: 0:11\n",
      "[TRAIN] Epoch: 73 Loss: 0.20296776741584274 Time: 1:17\n",
      "[VALID] Epoch: 73 Loss: 0.20026140808297008 Time: 0:11\n",
      "[TRAIN] Epoch: 74 Loss: 0.2020394501510797 Time: 1:17\n",
      "[VALID] Epoch: 74 Loss: 0.20315043480434092 Time: 0:11\n",
      "[TRAIN] Epoch: 75 Loss: 0.20355486932780273 Time: 1:18\n",
      "[VALID] Epoch: 75 Loss: 0.20001181846024477 Time: 0:11\n",
      "[TRAIN] Epoch: 76 Loss: 0.20264258175785121 Time: 1:18\n",
      "[VALID] Epoch: 76 Loss: 0.19962249216327 Time: 0:11\n",
      "[TRAIN] Epoch: 77 Loss: 0.20234702535483842 Time: 1:19\n",
      "[VALID] Epoch: 77 Loss: 0.2009712355234898 Time: 0:12\n",
      "[TRAIN] Epoch: 78 Loss: 0.20337324411812166 Time: 1:17\n",
      "[VALID] Epoch: 78 Loss: 0.20069892179699225 Time: 0:11\n",
      "[TRAIN] Epoch: 79 Loss: 0.2031837249346452 Time: 1:17\n",
      "[VALID] Epoch: 79 Loss: 0.20264884521784254 Time: 0:11\n",
      "[TRAIN] Epoch: 80 Loss: 0.20268444316353873 Time: 1:17\n",
      "[VALID] Epoch: 80 Loss: 0.2013979923391792 Time: 0:11\n",
      "[TRAIN] Epoch: 81 Loss: 0.20293710283248975 Time: 1:16\n",
      "[VALID] Epoch: 81 Loss: 0.20250109051738963 Time: 0:12\n",
      "[TRAIN] Epoch: 82 Loss: 0.20273466939645476 Time: 1:18\n",
      "[VALID] Epoch: 82 Loss: 0.20347300838805502 Time: 0:12\n",
      "[TRAIN] Epoch: 83 Loss: 0.20173224767208242 Time: 1:17\n",
      "[VALID] Epoch: 83 Loss: 0.2033681316510985 Time: 0:12\n",
      "[TRAIN] Epoch: 84 Loss: 0.2023971985930059 Time: 1:19\n",
      "[VALID] Epoch: 84 Loss: 0.19923647843745557 Time: 0:11\n",
      "[TRAIN] Epoch: 85 Loss: 0.20247028538897924 Time: 1:17\n",
      "[VALID] Epoch: 85 Loss: 0.20139599186170645 Time: 0:11\n",
      "[TRAIN] Epoch: 86 Loss: 0.20202496235215933 Time: 1:18\n",
      "[VALID] Epoch: 86 Loss: 0.20296351359236975 Time: 0:11\n",
      "[TRAIN] Epoch: 87 Loss: 0.20214499505450026 Time: 1:18\n",
      "[VALID] Epoch: 87 Loss: 0.20052067496811612 Time: 0:11\n",
      "[TRAIN] Epoch: 88 Loss: 0.20214706957677256 Time: 1:17\n",
      "[VALID] Epoch: 88 Loss: 0.20044040679704767 Time: 0:11\n",
      "[TRAIN] Epoch: 89 Loss: 0.2020094058627808 Time: 1:17\n",
      "[VALID] Epoch: 89 Loss: 0.20272162003166594 Time: 0:11\n",
      "[TRAIN] Epoch: 90 Loss: 0.20163248482665133 Time: 1:17\n",
      "[VALID] Epoch: 90 Loss: 0.20088747114764993 Time: 0:11\n",
      "[TRAIN] Epoch: 91 Loss: 0.20127556325377527 Time: 1:16\n",
      "[VALID] Epoch: 91 Loss: 0.20021483395927836 Time: 0:12\n",
      "[TRAIN] Epoch: 92 Loss: 0.2019989853070438 Time: 1:18\n",
      "[VALID] Epoch: 92 Loss: 0.2002307539481115 Time: 0:12\n",
      "[TRAIN] Epoch: 93 Loss: 0.20165882623299058 Time: 1:18\n",
      "[VALID] Epoch: 93 Loss: 0.2002301300987155 Time: 0:11\n",
      "[TRAIN] Epoch: 94 Loss: 0.2023975116709764 Time: 1:17\n",
      "[VALID] Epoch: 94 Loss: 0.20022936880214082 Time: 0:11\n",
      "[TRAIN] Epoch: 95 Loss: 0.20210021523160082 Time: 1:16\n",
      "[VALID] Epoch: 95 Loss: 0.2060859477932038 Time: 0:11\n",
      "[TRAIN] Epoch: 96 Loss: 0.2027142121080074 Time: 1:17\n",
      "[VALID] Epoch: 96 Loss: 0.20259659714653186 Time: 0:11\n",
      "[TRAIN] Epoch: 97 Loss: 0.20297939957845376 Time: 1:18\n",
      "[VALID] Epoch: 97 Loss: 0.20182912988738153 Time: 0:11\n",
      "[TRAIN] Epoch: 98 Loss: 0.20175293280740453 Time: 1:18\n",
      "[VALID] Epoch: 98 Loss: 0.1995123885848816 Time: 0:11\n",
      "[TRAIN] Epoch: 99 Loss: 0.201966056298537 Time: 1:17\n",
      "[VALID] Epoch: 99 Loss: 0.201283053120211 Time: 0:11\n",
      "[TRAIN] Epoch: 100 Loss: 0.20240095727333118 Time: 1:17\n",
      "[VALID] Epoch: 100 Loss: 0.19895637849942538 Time: 0:11\n",
      "[TRAIN] Epoch: 101 Loss: 0.20135777944856453 Time: 1:18\n",
      "[VALID] Epoch: 101 Loss: 0.20174680238503614 Time: 0:11\n",
      "[TRAIN] Epoch: 102 Loss: 0.20106530558567595 Time: 1:17\n",
      "[VALID] Epoch: 102 Loss: 0.1991064608362227 Time: 0:11\n",
      "[TRAIN] Epoch: 103 Loss: 0.2018396070588013 Time: 1:16\n",
      "[VALID] Epoch: 103 Loss: 0.20136675076108945 Time: 0:11\n",
      "[TRAIN] Epoch: 104 Loss: 0.2009379271142562 Time: 1:18\n",
      "[VALID] Epoch: 104 Loss: 0.2021137154746779 Time: 0:11\n",
      "[TRAIN] Epoch: 105 Loss: 0.20169387890256668 Time: 1:16\n",
      "[VALID] Epoch: 105 Loss: 0.20335449198049313 Time: 0:11\n",
      "[TRAIN] Epoch: 106 Loss: 0.20174582050971365 Time: 1:17\n",
      "[VALID] Epoch: 106 Loss: 0.20103099783674483 Time: 0:11\n",
      "[TRAIN] Epoch: 107 Loss: 0.20152353255633693 Time: 1:18\n",
      "[VALID] Epoch: 107 Loss: 0.20174120251175404 Time: 0:12\n",
      "[TRAIN] Epoch: 108 Loss: 0.20178137878611163 Time: 1:18\n",
      "[VALID] Epoch: 108 Loss: 0.2020815026687423 Time: 0:11\n",
      "[TRAIN] Epoch: 109 Loss: 0.20209316572008162 Time: 1:17\n",
      "[VALID] Epoch: 109 Loss: 0.20143146455644687 Time: 0:11\n",
      "[TRAIN] Epoch: 110 Loss: 0.20144810490954312 Time: 1:18\n",
      "[VALID] Epoch: 110 Loss: 0.2011339779221996 Time: 0:11\n",
      "[TRAIN] Epoch: 111 Loss: 0.20104662532334575 Time: 1:17\n",
      "[VALID] Epoch: 111 Loss: 0.1997372163750874 Time: 0:12\n",
      "[TRAIN] Epoch: 112 Loss: 0.20043579770150866 Time: 1:17\n",
      "[VALID] Epoch: 112 Loss: 0.20156333899138554 Time: 0:11\n",
      "[TRAIN] Epoch: 113 Loss: 0.201658372753119 Time: 1:17\n",
      "[VALID] Epoch: 113 Loss: 0.19985223984561598 Time: 0:11\n",
      "[TRAIN] Epoch: 114 Loss: 0.20159013939703854 Time: 1:17\n",
      "[VALID] Epoch: 114 Loss: 0.1989806272614707 Time: 0:11\n",
      "[TRAIN] Epoch: 115 Loss: 0.20134711357958626 Time: 1:17\n",
      "[VALID] Epoch: 115 Loss: 0.19923819161899128 Time: 0:11\n",
      "[TRAIN] Epoch: 116 Loss: 0.20171366899590448 Time: 1:17\n",
      "[VALID] Epoch: 116 Loss: 0.19936346636755956 Time: 0:11\n",
      "[TRAIN] Epoch: 117 Loss: 0.20166294578581032 Time: 1:16\n",
      "[VALID] Epoch: 117 Loss: 0.20017362349184037 Time: 0:12\n",
      "[TRAIN] Epoch: 118 Loss: 0.2012212155639069 Time: 1:18\n",
      "[VALID] Epoch: 118 Loss: 0.19898243473321667 Time: 0:11\n",
      "[TRAIN] Epoch: 119 Loss: 0.20141039543145628 Time: 1:17\n",
      "[VALID] Epoch: 119 Loss: 0.20070255565859535 Time: 0:11\n",
      "[TRAIN] Epoch: 120 Loss: 0.20155430238237468 Time: 1:18\n",
      "[VALID] Epoch: 120 Loss: 0.20301801128874297 Time: 0:11\n",
      "[TRAIN] Epoch: 121 Loss: 0.2019938702808199 Time: 1:17\n",
      "[VALID] Epoch: 121 Loss: 0.19926242267863223 Time: 0:11\n",
      "[TRAIN] Epoch: 122 Loss: 0.2013994652598418 Time: 1:18\n",
      "[VALID] Epoch: 122 Loss: 0.20008437804721485 Time: 0:11\n",
      "[TRAIN] Epoch: 123 Loss: 0.20146531859881403 Time: 1:16\n",
      "[VALID] Epoch: 123 Loss: 0.2003176478187161 Time: 0:12\n",
      "Epoch 00123: reducing learning rate of group 0 to 1.0000e-03.\n",
      "[TRAIN] Epoch: 124 Loss: 0.20035530721623668 Time: 1:18\n",
      "[VALID] Epoch: 124 Loss: 0.2003507278370703 Time: 0:11\n",
      "[TRAIN] Epoch: 125 Loss: 0.19967797666999643 Time: 1:18\n",
      "[VALID] Epoch: 125 Loss: 0.19699163671685443 Time: 0:11\n",
      "[TRAIN] Epoch: 126 Loss: 0.2006750758318432 Time: 1:17\n",
      "[VALID] Epoch: 126 Loss: 0.1997167242164847 Time: 0:11\n",
      "[TRAIN] Epoch: 127 Loss: 0.20013332028690284 Time: 1:19\n",
      "[VALID] Epoch: 127 Loss: 0.20059204217634224 Time: 0:11\n",
      "[TRAIN] Epoch: 128 Loss: 0.2004564539686775 Time: 1:18\n",
      "[VALID] Epoch: 128 Loss: 0.19870780175867364 Time: 0:11\n",
      "[TRAIN] Epoch: 129 Loss: 0.20001271817907695 Time: 1:17\n",
      "[VALID] Epoch: 129 Loss: 0.1982782084526341 Time: 0:11\n",
      "[TRAIN] Epoch: 130 Loss: 0.19996489384731064 Time: 1:16\n",
      "[VALID] Epoch: 130 Loss: 0.19763716148001162 Time: 0:11\n",
      "[TRAIN] Epoch: 131 Loss: 0.19977007267026725 Time: 1:18\n",
      "[VALID] Epoch: 131 Loss: 0.19698231578360897 Time: 0:11\n",
      "[TRAIN] Epoch: 132 Loss: 0.1998747294977818 Time: 1:17\n",
      "[VALID] Epoch: 132 Loss: 0.19821601939256314 Time: 0:11\n",
      "[TRAIN] Epoch: 133 Loss: 0.19972227785026872 Time: 1:16\n",
      "[VALID] Epoch: 133 Loss: 0.1982076816877077 Time: 0:11\n",
      "[TRAIN] Epoch: 134 Loss: 0.19952777918341272 Time: 1:17\n",
      "[VALID] Epoch: 134 Loss: 0.19666855185330967 Time: 0:11\n",
      "[TRAIN] Epoch: 135 Loss: 0.19924999404483695 Time: 1:16\n",
      "[VALID] Epoch: 135 Loss: 0.19895535627489938 Time: 0:11\n",
      "[TRAIN] Epoch: 136 Loss: 0.1994177893115901 Time: 1:17\n",
      "[VALID] Epoch: 136 Loss: 0.19730452387709885 Time: 0:11\n",
      "[TRAIN] Epoch: 137 Loss: 0.200258229314109 Time: 1:17\n",
      "[VALID] Epoch: 137 Loss: 0.19929890995059657 Time: 0:11\n",
      "[TRAIN] Epoch: 138 Loss: 0.20018395016894935 Time: 1:18\n",
      "[VALID] Epoch: 138 Loss: 0.1977622571995088 Time: 0:11\n",
      "[TRAIN] Epoch: 139 Loss: 0.1999867642290742 Time: 1:18\n",
      "[VALID] Epoch: 139 Loss: 0.1998138233139652 Time: 0:11\n",
      "[TRAIN] Epoch: 140 Loss: 0.19981645330337547 Time: 1:17\n",
      "[VALID] Epoch: 140 Loss: 0.19888038721264278 Time: 0:11\n",
      "[TRAIN] Epoch: 141 Loss: 0.19927452751624913 Time: 1:18\n",
      "[VALID] Epoch: 141 Loss: 0.20017597023635564 Time: 0:11\n",
      "[TRAIN] Epoch: 142 Loss: 0.19933139570841665 Time: 1:18\n",
      "[VALID] Epoch: 142 Loss: 0.19941394396786805 Time: 0:11\n",
      "[TRAIN] Epoch: 143 Loss: 0.19914478433178887 Time: 1:18\n",
      "[VALID] Epoch: 143 Loss: 0.19655827996101122 Time: 0:11\n",
      "[TRAIN] Epoch: 144 Loss: 0.1995954901445136 Time: 1:18\n",
      "[VALID] Epoch: 144 Loss: 0.20011676084617036 Time: 0:11\n",
      "[TRAIN] Epoch: 145 Loss: 0.19962809877777954 Time: 1:17\n",
      "[VALID] Epoch: 145 Loss: 0.19846603411803054 Time: 0:11\n",
      "[TRAIN] Epoch: 146 Loss: 0.1998953938654334 Time: 1:17\n",
      "[VALID] Epoch: 146 Loss: 0.19843807537171912 Time: 0:12\n",
      "[TRAIN] Epoch: 147 Loss: 0.19992946567229272 Time: 1:17\n",
      "[VALID] Epoch: 147 Loss: 0.1983394885517811 Time: 0:11\n",
      "[TRAIN] Epoch: 148 Loss: 0.19944158679213242 Time: 1:19\n",
      "[VALID] Epoch: 148 Loss: 0.19918633260357155 Time: 0:11\n",
      "[TRAIN] Epoch: 149 Loss: 0.20061788814967652 Time: 1:17\n",
      "[VALID] Epoch: 149 Loss: 0.19722501542131896 Time: 0:11\n",
      "[TRAIN] Epoch: 150 Loss: 0.2000009004213343 Time: 1:19\n",
      "[VALID] Epoch: 150 Loss: 0.19678293295937696 Time: 0:11\n",
      "Finish training !\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.27965382910751724,\n",
       "  0.22804690249232595,\n",
       "  0.22308680501612704,\n",
       "  0.21994440234572382,\n",
       "  0.21813957438734216,\n",
       "  0.21584955123358815,\n",
       "  0.21499654557658984,\n",
       "  0.21382891788195516,\n",
       "  0.2116843066521367,\n",
       "  0.21153840155606904,\n",
       "  0.21095573511796697,\n",
       "  0.21010204380159883,\n",
       "  0.21024271915627457,\n",
       "  0.2091244131206056,\n",
       "  0.20977675711085964,\n",
       "  0.20950613003866464,\n",
       "  0.2076683395381882,\n",
       "  0.20798420486048072,\n",
       "  0.20730620890115942,\n",
       "  0.20838954996560743,\n",
       "  0.2070062047126409,\n",
       "  0.2066266612769532,\n",
       "  0.20707193907971794,\n",
       "  0.2064464807652757,\n",
       "  0.20600506055348064,\n",
       "  0.20630110037791866,\n",
       "  0.20653259309149521,\n",
       "  0.20553451123523778,\n",
       "  0.20547243510280913,\n",
       "  0.20570724895928633,\n",
       "  0.20569670225876957,\n",
       "  0.20522272934446936,\n",
       "  0.20487745442883043,\n",
       "  0.2056912681900787,\n",
       "  0.20582109155035067,\n",
       "  0.20547539407042795,\n",
       "  0.20540634325798418,\n",
       "  0.20415691320301468,\n",
       "  0.20437575065805952,\n",
       "  0.20484396660495288,\n",
       "  0.20410320227074302,\n",
       "  0.2036458313160235,\n",
       "  0.204918595536757,\n",
       "  0.20451567301026397,\n",
       "  0.20482196938073066,\n",
       "  0.20452492209128917,\n",
       "  0.2041298785789316,\n",
       "  0.20460505310139424,\n",
       "  0.2038643462070462,\n",
       "  0.20451554649060336,\n",
       "  0.20340784254664876,\n",
       "  0.20430774581794456,\n",
       "  0.20436276942259618,\n",
       "  0.20431929952873115,\n",
       "  0.2036696982369021,\n",
       "  0.2041763900738263,\n",
       "  0.2032824233973009,\n",
       "  0.2042971259699703,\n",
       "  0.20331326093893531,\n",
       "  0.20407688127515855,\n",
       "  0.203718605709432,\n",
       "  0.20351935868764698,\n",
       "  0.20284875176274,\n",
       "  0.20250751828229682,\n",
       "  0.203315065991905,\n",
       "  0.20340544177048528,\n",
       "  0.20345558094089877,\n",
       "  0.20281927343576847,\n",
       "  0.20309760369216617,\n",
       "  0.20251744436825395,\n",
       "  0.2031944891674337,\n",
       "  0.2022092153875908,\n",
       "  0.20296776741584274,\n",
       "  0.2020394501510797,\n",
       "  0.20355486932780273,\n",
       "  0.20264258175785121,\n",
       "  0.20234702535483842,\n",
       "  0.20337324411812166,\n",
       "  0.2031837249346452,\n",
       "  0.20268444316353873,\n",
       "  0.20293710283248975,\n",
       "  0.20273466939645476,\n",
       "  0.20173224767208242,\n",
       "  0.2023971985930059,\n",
       "  0.20247028538897924,\n",
       "  0.20202496235215933,\n",
       "  0.20214499505450026,\n",
       "  0.20214706957677256,\n",
       "  0.2020094058627808,\n",
       "  0.20163248482665133,\n",
       "  0.20127556325377527,\n",
       "  0.2019989853070438,\n",
       "  0.20165882623299058,\n",
       "  0.2023975116709764,\n",
       "  0.20210021523160082,\n",
       "  0.2027142121080074,\n",
       "  0.20297939957845376,\n",
       "  0.20175293280740453,\n",
       "  0.201966056298537,\n",
       "  0.20240095727333118,\n",
       "  0.20135777944856453,\n",
       "  0.20106530558567595,\n",
       "  0.2018396070588013,\n",
       "  0.2009379271142562,\n",
       "  0.20169387890256668,\n",
       "  0.20174582050971365,\n",
       "  0.20152353255633693,\n",
       "  0.20178137878611163,\n",
       "  0.20209316572008162,\n",
       "  0.20144810490954312,\n",
       "  0.20104662532334575,\n",
       "  0.20043579770150866,\n",
       "  0.201658372753119,\n",
       "  0.20159013939703854,\n",
       "  0.20134711357958626,\n",
       "  0.20171366899590448,\n",
       "  0.20166294578581032,\n",
       "  0.2012212155639069,\n",
       "  0.20141039543145628,\n",
       "  0.20155430238237468,\n",
       "  0.2019938702808199,\n",
       "  0.2013994652598418,\n",
       "  0.20146531859881403,\n",
       "  0.20035530721623668,\n",
       "  0.19967797666999643,\n",
       "  0.2006750758318432,\n",
       "  0.20013332028690284,\n",
       "  0.2004564539686775,\n",
       "  0.20001271817907695,\n",
       "  0.19996489384731064,\n",
       "  0.19977007267026725,\n",
       "  0.1998747294977818,\n",
       "  0.19972227785026872,\n",
       "  0.19952777918341272,\n",
       "  0.19924999404483695,\n",
       "  0.1994177893115901,\n",
       "  0.200258229314109,\n",
       "  0.20018395016894935,\n",
       "  0.1999867642290742,\n",
       "  0.19981645330337547,\n",
       "  0.19927452751624913,\n",
       "  0.19933139570841665,\n",
       "  0.19914478433178887,\n",
       "  0.1995954901445136,\n",
       "  0.19962809877777954,\n",
       "  0.1998953938654334,\n",
       "  0.19992946567229272,\n",
       "  0.19944158679213242,\n",
       "  0.20061788814967652,\n",
       "  0.2000009004213343],\n",
       " [0.2313229511027656,\n",
       "  0.22015504617240078,\n",
       "  0.22393668344096215,\n",
       "  0.21574010761141812,\n",
       "  0.21390902532403008,\n",
       "  0.21409474290142302,\n",
       "  0.21471007284832788,\n",
       "  0.2141304829640659,\n",
       "  0.21150733639028707,\n",
       "  0.2116328423657865,\n",
       "  0.2093559718203824,\n",
       "  0.2128408599136288,\n",
       "  0.2100673151001363,\n",
       "  0.2065591656982777,\n",
       "  0.2100070858296373,\n",
       "  0.20767962058909709,\n",
       "  0.20788244669301914,\n",
       "  0.21104026339881263,\n",
       "  0.20925025782461018,\n",
       "  0.20571780326803024,\n",
       "  0.20848417047002984,\n",
       "  0.2056195543768884,\n",
       "  0.20579171575371866,\n",
       "  0.20777025873521263,\n",
       "  0.20665007197377716,\n",
       "  0.20603306665933038,\n",
       "  0.2040108160717708,\n",
       "  0.20544084034856794,\n",
       "  0.20869489277741868,\n",
       "  0.20663958267100463,\n",
       "  0.20334288071513557,\n",
       "  0.20689318299698234,\n",
       "  0.20548444527521934,\n",
       "  0.20200990569477517,\n",
       "  0.2015883042947186,\n",
       "  0.2029977062606211,\n",
       "  0.2058193798816914,\n",
       "  0.20516405893689463,\n",
       "  0.20429804643874994,\n",
       "  0.2050934084238146,\n",
       "  0.20385581718315782,\n",
       "  0.2072633088256019,\n",
       "  0.20145489519734042,\n",
       "  0.2022830337670151,\n",
       "  0.20374041940135532,\n",
       "  0.20377241041017277,\n",
       "  0.20384710365725628,\n",
       "  0.20358939728288752,\n",
       "  0.2064600384611052,\n",
       "  0.20299852197831625,\n",
       "  0.20197746929774768,\n",
       "  0.20208760004889362,\n",
       "  0.20378768459042154,\n",
       "  0.2029465092872875,\n",
       "  0.20171735404977226,\n",
       "  0.20106456866829134,\n",
       "  0.20303933281153755,\n",
       "  0.2012632138335716,\n",
       "  0.20503240961345184,\n",
       "  0.20549292191043278,\n",
       "  0.20359218394346132,\n",
       "  0.2003441208702996,\n",
       "  0.20337741097695813,\n",
       "  0.20261071704739367,\n",
       "  0.20412063988593265,\n",
       "  0.2016973477351885,\n",
       "  0.2037554281318357,\n",
       "  0.20215047498956973,\n",
       "  0.20099950252619372,\n",
       "  0.20048279764059426,\n",
       "  0.20161593485747695,\n",
       "  0.2039492793054515,\n",
       "  0.20026140808297008,\n",
       "  0.20315043480434092,\n",
       "  0.20001181846024477,\n",
       "  0.19962249216327,\n",
       "  0.2009712355234898,\n",
       "  0.20069892179699225,\n",
       "  0.20264884521784254,\n",
       "  0.2013979923391792,\n",
       "  0.20250109051738963,\n",
       "  0.20347300838805502,\n",
       "  0.2033681316510985,\n",
       "  0.19923647843745557,\n",
       "  0.20139599186170645,\n",
       "  0.20296351359236975,\n",
       "  0.20052067496811612,\n",
       "  0.20044040679704767,\n",
       "  0.20272162003166594,\n",
       "  0.20088747114764993,\n",
       "  0.20021483395927836,\n",
       "  0.2002307539481115,\n",
       "  0.2002301300987155,\n",
       "  0.20022936880214082,\n",
       "  0.2060859477932038,\n",
       "  0.20259659714653186,\n",
       "  0.20182912988738153,\n",
       "  0.1995123885848816,\n",
       "  0.201283053120211,\n",
       "  0.19895637849942538,\n",
       "  0.20174680238503614,\n",
       "  0.1991064608362227,\n",
       "  0.20136675076108945,\n",
       "  0.2021137154746779,\n",
       "  0.20335449198049313,\n",
       "  0.20103099783674483,\n",
       "  0.20174120251175404,\n",
       "  0.2020815026687423,\n",
       "  0.20143146455644687,\n",
       "  0.2011339779221996,\n",
       "  0.1997372163750874,\n",
       "  0.20156333899138554,\n",
       "  0.19985223984561598,\n",
       "  0.1989806272614707,\n",
       "  0.19923819161899128,\n",
       "  0.19936346636755956,\n",
       "  0.20017362349184037,\n",
       "  0.19898243473321667,\n",
       "  0.20070255565859535,\n",
       "  0.20301801128874297,\n",
       "  0.19926242267863223,\n",
       "  0.20008437804721485,\n",
       "  0.2003176478187161,\n",
       "  0.2003507278370703,\n",
       "  0.19699163671685443,\n",
       "  0.1997167242164847,\n",
       "  0.20059204217634224,\n",
       "  0.19870780175867364,\n",
       "  0.1982782084526341,\n",
       "  0.19763716148001162,\n",
       "  0.19698231578360897,\n",
       "  0.19821601939256314,\n",
       "  0.1982076816877077,\n",
       "  0.19666855185330967,\n",
       "  0.19895535627489938,\n",
       "  0.19730452387709885,\n",
       "  0.19929890995059657,\n",
       "  0.1977622571995088,\n",
       "  0.1998138233139652,\n",
       "  0.19888038721264278,\n",
       "  0.20017597023635564,\n",
       "  0.19941394396786805,\n",
       "  0.19655827996101122,\n",
       "  0.20011676084617036,\n",
       "  0.19846603411803054,\n",
       "  0.19843807537171912,\n",
       "  0.1983394885517811,\n",
       "  0.19918633260357155,\n",
       "  0.19722501542131896,\n",
       "  0.19678293295937696])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the network\n",
    "tr = train.Train()\n",
    "tr.run(train_ds, show_log=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086d418a-308b-42db-a56d-9cef2a27c706",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
